## Introdução: Hardware (seção 1.3)

### Resumo

#### CPU: funcionamento básico e necessidades para a implementação de um SO.

Evolução da CPU: sequencial, pipeline, superescalar, multithread, multicore.

Caso de 1 CPU: a mesma CPU é usada para executar o SO e para executar aplicações de usuário. Quando a CPU está executando um programa do usuário, em sistemas mais simples ele pode assumir controle do computador. Caso se queira garantias de que o sistema operacional não perca controle do computador, é necessário ajuda do hardware. Uma das medidas é restringir a execução de algumas instruções (E/S, por exemplo).

Isso normalmente é resolvido com a implementação de dois modos de execução, modo usuário (em que o processador não executa instruções restritas) e modo supervisor (que pode executar qualquer instrução).

Tem que ter uma forma controlada de se passar de um modo para outro, para garantir que um programa executando em modo usuário não consiga passar para modo supervisor e executar seu código. A forma encontrada foi associar a mudança para modo supervisor ao desvio da execução para código confiável (pertencente ao SO), de forma semelhante ao atendimento de uma interrupção de hardware (por vezes é chamado de interrupção por software).

Tem que ter uma forma de garantir que o código interrompido por uma interrupção possa ter sua execução continuada “como se nada tivesse acontecido”, depois que a interrupção for tratada. 
Para isso, o estado da CPU deve ser salvo quando se inicia o tratamento de uma interrupção e recuperado quando a execução do código interrompido for continuar.
Para simplificar, geralmente se implementa a CPU de forma a que se tenha um ponto de execução único: até tal instrução (do ponto de vista do código que está sendo executado), todas as instruções já foram completamente executadas, a partir desse ponto, nenhuma instrução foi executada.
Isso implica na perda do estado do pipeline, e na perda de instruções executadas fora de ordem (em um superescalar).

Precisa também de uma forma de se proteger o conteúdo da memória – no mínimo, não deve ser possível alterar o conteúdo da memória usada pelo sistema operacional quando a CPU não estiver no modo supervisor.


#### Memória

Para que o processador possa ter acesso a algum dado, esse dado deve estar em memória.
A execução de cada instrução pelo processador exige o acesso a pelo menos um dado da memória: o código da própria instrução, e potencialmente mais, quando a execução da instrução necessita de algum dado (o que é o caso da maior parte das instruções – uma instrução aritmética típica necessita dois dados sobre os quais operar e produz um novo como resultado).
A quantidade de memória disponível é um fator importante na capacidade de processamento de um computador.

Comparado a um processador, a memória é um dispositivo relativamente simples.
Por isso, durante a evolução dos computadores encontrou-se bem mais oportunidades de melhorias de desempenho nos processadores que nas memórias.
Isso fez com que a velocidade relativa entre eles favorecesse cada vez mais os processadores.
Nos primeiros computadores, a velocidade da memória não era um problema, então era normal a execução de uma instrução realizar vários acessos à memória.
Conforme o tempo de acesso à memória foi se transformando em um gargalo para o desempenho dos processadores, foram criadas formas de aliviar esse problema.
Uma dessas formas é uma mudança arquitetural (em direção à arquitetura RISC), aumentando-se o número de registradores e diminuindo o número de instruções que acessam a memória, dando preferência para instruções que operam sobre os registradores.

Outra mudança foi o desenvolvimento de memórias mais rápidas (e mais caras), que se coloca em quantidade reduzida, na forma de memória cache.
O uso desse tipo de memória é efetivo porque os programas apresentam características de localidade temporal (os programas tipicamente acessam os mesmos dados diversas vezes em pouco tempo) e localidade espacial (os programas tipicamente acessas dados fisicamente próximos em cada fase de sua execução).
Em processadores atuais, é comum a existência de 3 níveis de memória cache.

Um terceiro desenvolvimento não tem a ver com velocidade mas com o aumento da capacidade da memória, que é memória virtual.
A idéia é manter na memória principal somente os dados que estão sendo utilizados em um dado momento, e em um dispositivo de armazenamento os dados que não estão sendo utilizados no momento, mas podem sê-lo mais tarde.
O espaço em dispositivos de armazenamento usado para aumentar a capacidade efetiva da memória é chamado de memória secundária.
Essa idéia pode ser usada entre programas diferentes (retira da memória principal os programas que não estão em execução) ou no interior de um mesmo programa, retirando da memória principal partes de um programa que não estejam sendo utilizadas, aproveitando as características de localidade temporal dos programas.

Isso tudo criou uma hierarquia de memória, formada por registradores, memória cache, memória principal e memória secundária.
Quanto mais próximo do processador nessa hierarquia, mais rápida, mais cara e de menor capacidade é a memória.
Em cada nível dessa hierarquia, tem que ser tomada a decisão de quais dados ficam nesse nível e quais vão para os níveis mais abaixo.
A decisão sobre os registradores é tomada pelo compilador, e é uma das fases mais importantes de otimização durante a compilação.
A decisão sobre as caches é tomada pelo hardware que implementa a memória cache.
Não é uma decisão que possa ser tomada em software, porque deve ser tomada a cada acesso à memória.
A diferença de desempenho entre a memória principal e a secundária é tão grande que justifica gastar mais tempo para tentar implementar um algoritmo mais elaborado para essa decisão.
Esse algoritmo é executado pelo sistema operacional.

Uma outra tarefa do sistema operacional em relação à memória é a divisão da memória disponível entre os programas que estão sendo executados, e a proteção dessa memória (não permitir que um desses programas afete a memória que não lhe pertence).
Para implementar a proteção, o SO necessita da ajuda do hardware, tipicamente um módulo, configurável pelo SO, chamado MMU (unidade de gerenciamento de memória), que fica entre a CPU e o sistema de memória e verifica se cada acesso à memória é permitido ou não.


#### Entrada e saída (E/S)

Existe uma grande variedade de dispositivos de E/S disponíveis para um sistema de computação, com requisitos muito diferentes entre eles, em termos de velocidade e volume de dados que são transferidos.
Para simplificar e unificar o acesso a esses diferentes dispositivos, o SO deve criar abstrações mais simples para representá-los.
Além disso, o SO precisa poder garantir que ele é o único software que realiza operações sobre esses dispositivos diretamente, para garantir que funcionem adequadamente e de acordo com as permissões de acesso.

A comunicação entre a CPU e um dispositivo de E/S tipicamente é de 3 tipos: envio de controle para o dispositivo (para dizer o que se quer que o dispositivo faça), envio de informações de estado do dispositivo para a CPU (para saber se o dispositivo pode receber um pedido ou se terminou algum pedido anterior) e informações de dados, que pode ser em um ou outro sentido, dependendo da operação que está sendo realizada.

Tem duas formas comumente utilizadas para essas transferências: mapeamento em memória e espaço de endereçamento de E/S. Na primeira, o espaço de memória (o conjunto de endereços que podem ser gerados pelo processador) é separado em duas regiões, a de memória e a de E/S, e os dispositivos de E/S têm acesso ao barramento de memória. As instruções utilizadas pelo processador para se comunicar com dispositivos de E/S são as mesmas que ele utiliza para acessar a memória, somente em endereços específicos para cada dispositivo. O controle de acesso nesse caso tem que ser implementado pelo mesmo mecanismo de controle de acesso à memória.

No caso de barramento de E/S, a CPU tem dois baramentos distintos, um para comunicação com a memória e outro para a comunicação com E/S.
Existem instruções diferentes para acesso a cada barramento.
O controle de acesso nesse caso é implementado classificando as instruções de E/S como instruções privilegiadas, que a CPU não executa quando está em modo usuário.

Os dispositivos de E/S são geralmente bem mais lentos que a CPU, não conseguindo fornecer ou receber dados na mesma velocidade que a CPU.
Uma forma de se casar essa diferença é a CPU realizar o controle fino de cada transferência, o que se traduz em ficar em um laço lendo o estado do dispositivo até que ele informa que está pronto, então fazer parte da transferência e então ficar novamente esperando até que a próxima parte possa ser executada e assim por diante até o término da operação.
Essa forma de transferência causa um problema chamado de espera ocupada; tem a vantagem de a CPU poder reagir muito rapidamente a um dispositivo de E/S, mas tem a grande desvantagem de manter a CPU sem realizar nenhum trabalho útil enquanto aguarda E/S.

Uma forma alternativa de sincronizar a CPU com a E/S é ligando-se o sinal de término de uma operação (ou outro sinal de estado considerado importante) a uma linha de interrupção.
Nessa forma, a CPU transfere ao dispositivo de E/S toda a informação necessária para que ele realize uma operação e vai fazer outra coisa (tipicamente executar outro programa).
Quando o dispositivo sinalizar que a operação está terminada, a CPU é interrompida, causando a execução do SO, que pode continuar o papo com o dispositivo, se for o caso.

Quando o dispositivo tem um volume de dados considerável, uma operação pode resultar em um grande número de interrupções, cada uma para transferência de parte dos dados.
Uma alternativa para se realizar a transferência de dados nesses casos é permitir ao dispositivo acessar o barramento da memória e ele mesmo realizar a transferência.
Nesse caso, uma das informações passadas pela CPU ao dispositivo de E/S é o endereço onde estão ou onde devem ser colocados os dados.
Quando a CPU receber a interrupção, toda a transferência já vai ter sido realizada.
Essa forma de transferência é chamada de DMA (acesso direto à memória).
